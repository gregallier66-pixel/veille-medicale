import streamlit as st
import google.generativeai as genai
import requests
import json
from datetime import datetime, date, timedelta
import xml.etree.ElementTree as ET
from fpdf import FPDF
import io

st.set_page_config(page_title="Veille M√©dicale Pro", layout="wide")

# R√©cup√©ration de la cl√© Gemini
try:
    G_KEY = st.secrets["GEMINI_KEY"]
except:
    st.error("‚ö†Ô∏è Cl√© GEMINI_KEY manquante dans les secrets")
    st.stop()

# Sp√©cialit√©s √©tendues
TRAD = {
    "Gyn√©cologie": "Gynecology",
    "Endocrinologie": "Endocrinology",
    "M√©decine G√©n√©rale": "General Medicine",
    "Cardiologie": "Cardiology",
    "Neurologie": "Neurology",
    "Oncologie": "Oncology",
    "P√©diatrie": "Pediatrics",
    "Anesth√©sie-R√©animation": "Anesthesiology",
    "Obst√©trique": "Obstetrics"
}

# Types d'√©tudes
TYPES_ETUDE = {
    "Tous": "",
    "Essais cliniques": "Clinical Trial",
    "M√©ta-analyses": "Meta-Analysis",
    "Revues syst√©matiques": "Systematic Review",
    "√âtudes de cohorte": "Cohort Studies",
    "√âtudes cas-t√©moins": "Case-Control Studies"
}

# Journaux par sp√©cialit√©
JOURNAUX_SPECIALITE = {
    "Gyn√©cologie": ["BJOG", "Obstet Gynecol", "Am J Obstet Gynecol", "Hum Reprod", "Fertil Steril"],
    "Obst√©trique": ["BJOG", "Obstet Gynecol", "Am J Obstet Gynecol", "Ultrasound Obstet Gynecol"],
    "Endocrinologie": ["J Clin Endocrinol Metab", "Diabetes Care", "Eur J Endocrinol", "Endocr Rev"],
    "Cardiologie": ["Circulation", "JACC", "Eur Heart J", "J Am Coll Cardiol", "Heart"],
    "Neurologie": ["Neurology", "Brain", "Lancet Neurol", "JAMA Neurol", "Ann Neurol"],
    "Oncologie": ["J Clin Oncol", "Lancet Oncol", "Cancer", "JAMA Oncol", "Ann Oncol"],
    "P√©diatrie": ["Pediatrics", "JAMA Pediatr", "Arch Dis Child", "J Pediatr"],
    "Anesth√©sie-R√©animation": ["Anesthesiology", "Br J Anaesth", "Anesth Analg", "Intensive Care Med"],
    "M√©decine G√©n√©rale": ["BMJ", "JAMA", "N Engl J Med", "Lancet", "Ann Intern Med"]
}

# Sources suppl√©mentaires
SOURCES_SUPPLEMENTAIRES = {
    "HAS (Haute Autorit√© de Sant√©)": "https://www.has-sante.fr",
    "CNGOF (Coll√®ge National des Gyn√©cologues et Obst√©triciens Fran√ßais)": "http://www.cngof.fr",
    "Vidal": "https://www.vidal.fr",
    "Cochrane Library": "https://www.cochranelibrary.com",
    "UpToDate": "https://www.uptodate.com"
}

# Initialiser l'historique
if 'historique' not in st.session_state:
    st.session_state.historique = []

# Fonction pour parser une date au format dd/mm/yyyy
def parse_date_fr(date_str):
    """Convertit une date dd/mm/yyyy en objet date"""
    try:
        return datetime.strptime(date_str, "%d/%m/%Y").date()
    except:
        return None

# Fonction pour formater une date en dd/mm/yyyy
def format_date_fr(date_obj):
    """Convertit un objet date en dd/mm/yyyy"""
    return date_obj.strftime("%d/%m/%Y")

# Fonction pour r√©cup√©rer le lien PDF
def get_pdf_link(pmid):
    """R√©cup√®re le lien du PDF en libre acc√®s"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi"
        params = {
            "dbfrom": "pubmed",
            "db": "pmc",
            "id": pmid,
            "retmode": "xml"
        }
        
        response = requests.get(base_url, params=params, timeout=10)
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            pmc_id = root.find('.//Link/Id')
            
            if pmc_id is not None:
                pmc_id_text = pmc_id.text
                pdf_url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id_text}/pdf/"
                return pdf_url, pmc_id_text
        
        return None, None
    except:
        return None, None

# Fonction pour v√©rifier les mots-cl√©s
def verifier_mots_cles_pubmed(mots_cles):
    """V√©rifie si les mots-cl√©s existent dans PubMed"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            "db": "pubmed",
            "term": mots_cles,
            "retmode": "json",
            "retmax": "1"
        }
        response = requests.get(base_url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            count = int(data.get("esearchresult", {}).get("count", "0"))
            return count > 0, count
        return False, 0
    except:
        return None, 0

# Fonction pour traduire les mots-cl√©s
def traduire_mots_cles(mots_cles_fr, api_key):
    """Traduit les mots-cl√©s fran√ßais en anglais"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        prompt = f"""Traduis ces mots-cl√©s m√©dicaux fran√ßais en anglais m√©dical pour PubMed.
Retourne UNIQUEMENT les termes anglais.

Mots-cl√©s fran√ßais: {mots_cles_fr}

Termes anglais:"""
        
        response = model.generate_content(prompt)
        return response.text.strip()
    except:
        return mots_cles_fr

# Fonction pour traduire un texte
def traduire_texte(texte, api_key):
    """Traduit un texte en fran√ßais"""
    if not texte or texte == "R√©sum√© non disponible":
        return texte
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        prompt = f"""Traduis ce texte m√©dical en fran√ßais de mani√®re professionnelle.

Texte:
{texte}

Traduction:"""
        
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        if "429" in str(e) or "quota" in str(e).lower():
            return f"[Quota d√©pass√©]\n\n{texte}"
        return f"[Erreur]\n\n{texte}"

# Fonction PDF
class PDF(FPDF):
    def header(self):
        self.set_font('Arial', 'B', 16)
        self.cell(0, 10, 'Veille Medicale', 0, 1, 'C')
        self.ln(5)
    
    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')
    
    def section_title(self, title):
        self.set_font('Arial', 'B', 14)
        self.set_fill_color(200, 220, 255)
        self.cell(0, 10, title, 0, 1, 'L', 1)
        self.ln(3)

def generer_pdf_complet(spec, periode, nb_articles, pmids, synthese, articles_data):
    """G√©n√®re un PDF complet"""
    pdf = PDF()
    pdf.add_page()
    
    pdf.set_font('Arial', 'B', 20)
    pdf.ln(30)
    pdf.cell(0, 15, 'VEILLE MEDICALE', 0, 1, 'C')
    pdf.ln(20)
    
    pdf.set_font('Arial', '', 12)
    pdf.cell(0, 8, f'Specialite: {spec}', 0, 1, 'C')
    pdf.cell(0, 8, f'Periode: {periode}', 0, 1, 'C')
    pdf.cell(0, 8, f'Articles: {nb_articles}', 0, 1, 'C')
    pdf.cell(0, 8, f'Date: {datetime.now().strftime("%d/%m/%Y")}', 0, 1, 'C')
    
    pdf.add_page()
    pdf.section_title('SYNTHESE IA')
    
    try:
        synthese_clean = synthese.encode('latin-1', 'ignore').decode('latin-1')
    except:
        synthese_clean = synthese.encode('ascii', 'ignore').decode('ascii')
    
    pdf.set_font('Arial', '', 10)
    pdf.multi_cell(0, 5, synthese_clean)
    
    pdf.add_page()
    pdf.section_title('ARTICLES')
    
    for i, article in enumerate(articles_data, 1):
        pdf.set_font('Arial', 'B', 11)
        pdf.cell(0, 8, f'Article {i} - PMID: {article["pmid"]}', 0, 1)
        
        pdf.set_font('Arial', '', 10)
        try:
            title_clean = article['title'].encode('latin-1', 'ignore').decode('latin-1')
        except:
            title_clean = article['title'].encode('ascii', 'ignore').decode('ascii')
        pdf.multi_cell(0, 5, title_clean)
        pdf.ln(2)
        
        try:
            abstract_clean = article['abstract_fr'].encode('latin-1', 'ignore').decode('latin-1')
        except:
            abstract_clean = article['abstract_fr'].encode('ascii', 'ignore').decode('ascii')
        pdf.multi_cell(0, 4, abstract_clean)
        pdf.ln(5)
    
    pdf_output = io.BytesIO()
    pdf_string = pdf.output(dest='S').encode('latin-1')
    pdf_output.write(pdf_string)
    pdf_output.seek(0)
    
    return pdf_output.getvalue()

def recuperer_abstracts(pmids, traduire=False, api_key=None):
    """R√©cup√®re les r√©sum√©s depuis PubMed"""
    base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    
    params = {
        "db": "pubmed",
        "id": ",".join(pmids),
        "retmode": "xml",
        "rettype": "abstract"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=15)
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            articles_data = []
            
            for article in root.findall('.//PubmedArticle'):
                pmid = article.find('.//PMID').text if article.find('.//PMID') is not None else "N/A"
                
                title_elem = article.find('.//ArticleTitle')
                title = title_elem.text if title_elem is not None else "Titre non disponible"
                
                abstract_elem = article.find('.//AbstractText')
                abstract = abstract_elem.text if abstract_elem is not None else "R√©sum√© non disponible"
                
                abstract_fr = abstract
                if traduire and abstract != "R√©sum√© non disponible" and api_key:
                    abstract_fr = traduire_texte(abstract, api_key)
                
                authors = []
                for author in article.findall('.//Author'):
                    lastname = author.find('LastName')
                    forename = author.find('ForeName')
                    if lastname is not None:
                        name = lastname.text
                        if forename is not None:
                            name = f"{forename.text} {name}"
                        authors.append(name)
                
                journal_elem = article.find('.//Journal/Title')
                journal = journal_elem.text if journal_elem is not None else "Journal non disponible"
                
                year_elem = article.find('.//PubDate/Year')
                year = year_elem.text if year_elem is not None else "N/A"
                
                articles_data.append({
                    'pmid': pmid,
                    'title': title,
                    'abstract': abstract,
                    'abstract_fr': abstract_fr,
                    'authors': authors,
                    'journal': journal,
                    'year': year
                })
            
            return articles_data
    except Exception as e:
        st.warning(f"Erreur: {str(e)}")
        return []
    
    return []

def generer_fichier_notebooklm(synthese, articles_data):
    """G√©n√®re un fichier pour NotebookLM"""
    contenu = f"""# VEILLE M√âDICALE - SYNTH√àSE POUR PODCAST
Date: {datetime.now().strftime("%d/%m/%Y")}

## SYNTH√àSE PRINCIPALE

{synthese}

## ARTICLES SOURCES

"""
    
    for i, article in enumerate(articles_data, 1):
        contenu += f"""
### Article {i}
**Titre:** {article['title']}
**Auteurs:** {', '.join(article['authors'][:5])}
**Journal:** {article['journal']} ({article['year']})
**PMID:** {article['pmid']}

**R√©sum√©:**
{article['abstract_fr']}

---
"""
    
    return contenu

def sauvegarder_recherche(spec, periode, type_etude, langue, pmids, synthese, mots_cles=""):
    """Sauvegarde la recherche"""
    recherche = {
        'date': datetime.now().strftime("%d/%m/%Y %H:%M"),
        'specialite': spec,
        'periode': periode,
        'type_etude': type_etude,
        'langue': langue,
        'mots_cles': mots_cles,
        'nb_articles': len(pmids),
        'pmids': pmids,
        'synthese': synthese
    }
    st.session_state.historique.insert(0, recherche)
    if len(st.session_state.historique) > 20:
        st.session_state.historique = st.session_state.historique[:20]

# Interface principale
st.title("ü©∫ Veille M√©dicale Professionnelle")
st.markdown("*Analyse avanc√©e des publications PubMed avec IA*")

tab1, tab2, tab3 = st.tabs(["üîç Recherche", "üìö Historique", "üîó Sources"])

with tab1:
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        mode_recherche = st.radio("Mode", ["Par sp√©cialit√©", "Par mots-cl√©s"], horizontal=True)
        
        if mode_recherche == "Par sp√©cialit√©":
            spec_fr = st.selectbox("üè• Sp√©cialit√©", list(TRAD.keys()))
            mots_cles_custom = ""
            mots_cles_originaux = ""
            
            st.subheader("üì∞ Journal (optionnel)")
            journaux_dispo = ["Tous"] + JOURNAUX_SPECIALITE.get(spec_fr, [])
            journal_selectionne = st.selectbox("Journal", journaux_dispo)
            
        else:
            spec_fr = None
            journal_selectionne = "Tous"
            
            inclure_specialite = st.checkbox("üî¨ Inclure sp√©cialit√©", value=False)
            if inclure_specialite:
                spec_combo = st.selectbox("Sp√©cialit√©", list(TRAD.keys()))
            else:
                spec_combo = None
            
            mots_cles_custom = st.text_area(
                "üîé Mots-cl√©s",
                placeholder="Ex: diab√®te gestationnel",
                height=80
            )
            mots_cles_originaux = mots_cles_custom
            
            if mots_cles_custom:
                if st.button("üîç V√©rifier"):
                    with st.spinner("V√©rification..."):
                        mots_cles_en = traduire_mots_cles(mots_cles_custom, G_KEY)
                        existe, count = verifier_mots_cles_pubmed(mots_cles_en)
                        
                        if existe:
                            st.success(f"‚úÖ {count:,} articles")
                        else:
                            st.warning("‚ö†Ô∏è Aucun article")
        
        st.subheader("üéØ Zone")
        zone_recherche = st.radio(
            "Chercher dans:",
            ["Titre et r√©sum√©", "Titre uniquement", "R√©sum√© uniquement"]
        )
        
        st.subheader("üìÖ P√©riode")
        
        col1, col2 = st.columns(2)
        
        with col1:
            date_debut_input = st.text_input(
                "D√©but (JJ/MM/AAAA)",
                value="01/01/2024"
            )
            date_debut = parse_date_fr(date_debut_input)
            if not date_debut:
                st.error("Format invalide")
                date_debut = date(2024, 1, 1)
        
        with col2:
            date_fin_input = st.text_input(
                "Fin (JJ/MM/AAAA)",
                value=format_date_fr(date.today())
            )
            date_fin = parse_date_fr(date_fin_input)
            if not date_fin:
                st.error("Format invalide")
                date_fin = date.today()
        
        st.subheader("üîì Acc√®s")
        acces_libre = st.checkbox("üìñ PDF gratuit uniquement", value=False)
        
        st.subheader("üî¨ Filtres")
        type_etude = st.selectbox("Type", list(TYPES_ETUDE.keys()))
        
        langue = st.selectbox("Langue", [
            "Toutes",
            "Anglais",
            "Fran√ßais",
            "Espagnol"
        ])
        
        traduire_abstracts = st.checkbox("üåê Traduire", value=True)
        
        nb = st.slider("üìä Articles", 1, 20, 5)

    if st.button("üîç Lancer", type="primary", use_container_width=True):
        
        if mode_recherche == "Par mots-cl√©s" and not mots_cles_custom:
            st.error("‚ö†Ô∏è Entrez des mots-cl√©s")
            st.stop()
        
        if date_debut > date_fin:
            st.error("‚ö†Ô∏è P√©riode invalide")
            st.stop()
        
        if mode_recherche == "Par sp√©cialit√©":
            term = TRAD[spec_fr]
            display_term = spec_fr
        else:
            with st.spinner("üåê Traduction..."):
                term = traduire_mots_cles(mots_cles_custom, G_KEY)
            
            if inclure_specialite and spec_combo:
                term = f"{term} AND {TRAD[spec_combo]}"
            
            display_term = f"Mots-cl√©s: {mots_cles_custom}"
            st.info(f"üîÑ {term}")
        
        query_parts = [term]
        
        if zone_recherche == "Titre uniquement":
            query_parts[0] = f"{query_parts[0]}[Title]"
        elif zone_recherche == "R√©sum√© uniquement":
            query_parts[0] = f"{query_parts[0]}[Abstract]"
        
        date_debut_pubmed = date_debut.strftime("%Y/%m/%d")
        date_fin_pubmed = date_fin.strftime("%Y/%m/%d")
        query_parts.append(f"{date_debut_pubmed}:{date_fin_pubmed}[pdat]")
        
        if acces_libre:
            query_parts.append("free full text[sb]")
        
        if journal_selectionne != "Tous":
            query_parts.append(f'"{journal_selectionne}"[Journal]')
        
        if TYPES_ETUDE[type_etude]:
            query_parts.append(f"{TYPES_ETUDE[type_etude]}[ptyp]")
        
        langue_codes = {"Anglais": "eng", "Fran√ßais": "fre", "Espagnol": "spa"}
        if langue != "Toutes":
            query_parts.append(f"{langue_codes[langue]}[la]")
        
        query = " AND ".join(query_parts)
        
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        
        params = {
            "db": "pubmed",
            "term": query,
            "retmode": "json",
            "retmax": nb,
            "sort": "relevance"
        }
        
        periode_affichage = f"du {format_date_fr(date_debut)} au {format_date_fr(date_fin)}"
        
        try:
            with st.spinner("üîé Recherche..."):
                response = requests.get(
                    base_url,
                    params=params,
                    headers={'User-Agent': 'Streamlit App'},
                    timeout=15
                )
            
            if response.status_code != 200:
                st.error(f"‚ùå Erreur: {response.status_code}")
                st.stop()
            
            data = response.json()
            search_result = data.get("esearchresult", {})
            ids = search_result.get("idlist", [])
            count = search_result.get("count", "0")
            
            if not ids:
                st.warning("‚ö†Ô∏è Aucun article trouv√©")
                st.stop()
            
            st.success(f"‚úÖ {count} articles - Affichage de {len(ids)}")
            
            with st.spinner("üìÑ R√©cup√©ration..."):
                articles_complets = recuperer_abstracts(ids, traduire=traduire_abstracts, api_key=G_KEY)
            
            if articles_complets:
                st.subheader("üìö Articles")
                
                for i, article in enumerate(articles_complets, 1):
                    with st.expander(f"**Article {i}** - {article['title'][:80]}..."):
                        st.markdown(f"**PMID:** [{article['pmid']}](https://pubmed.ncbi.nlm.nih.gov/{article['pmid']}/)")
                        st.markdown(f"**Journal:** {article['journal']} ({article['year']})")
                        
                        if traduire_abstracts:
                            st.markdown("**üìñ R√©sum√© (FR):**")
                            st.write(article['abstract_fr'])
                        else:
                            st.markdown("**üìñ R√©sum√©:**")
                            st.write(article['abstract'])
                        
                        if acces_libre:
                            st.divider()
                            pdf_url, pmc_id = get_pdf_link(article['pmid'])
                            
                            if pdf_url:
                                st.markdown("**üìÑ PDF disponible**")
                                st.link_button("üì• Acc√©der au PDF", pdf_url)
                            else:
                                st.info("PDF non disponible en libre acc√®s")
            
            st.divider()
            st.subheader("ü§ñ Synth√®se IA")
            
            with st.spinner("‚è≥ Analyse..."):
                try:
                    genai.configure(api_key=G_KEY)
                    model = genai.GenerativeModel('gemini-2.5-flash')
                    
                    contexte = ""
                    if articles_complets:
                        for art in articles_complets:
                            resume = art['abstract_fr'] if traduire_abstracts else art['abstract']
                            contexte += f"\n\nPMID {art['pmid']}:\n{art['title']}\n{resume}\n"
                    
                    liens = "\n".join([f"- https://pubmed.ncbi.nlm.nih.gov/{pmid}/" for pmid in ids])
                    
                    spec_texte = spec_fr if mode_recherche == "Par sp√©cialit√©" else f"Mots-cl√©s: {mots_cles_custom}"
                    
                    prompt = f"""Expert m√©dical - Veille.

{len(ids)} articles PubMed.

**Crit√®res:** {spec_texte} | {periode_affichage} | {type_etude}

**Articles:**
{contexte}

**PMIDs:** {', '.join(ids)}

Synth√®se fran√ßaise:

## üìä Vue d'ensemble
## üî¨ Tendances
## üí° D√©couvertes
## üè• Implications
## ‚ö†Ô∏è Limites

## üîó Sources
{liens}"""
                    
                    response_ia = model.generate_content(prompt)
                    synthese = response_ia.text
                    
                    st.markdown(synthese)
                    
                    # Section Podcast NotebookLM
                    st.divider()
                    st.subheader("üéôÔ∏è Cr√©er un Podcast Audio")
                    
                    st.info("""
üí° **G√©n√©rer un podcast automatiquement avec NotebookLM :**
1. T√©l√©chargez le fichier optimis√© ci-dessous
2. Allez sur [NotebookLM](https://notebooklm.google.com)
3. Cr√©ez un nouveau notebook
4. Importez le fichier t√©l√©charg√©
5. Cliquez sur "Generate Audio Overview"
6. Un podcast conversationnel sera cr√©√© automatiquement (dur√©e : 5-15 minutes selon le contenu)
                    """)
                    
                    fichier_notebooklm = generer_fichier_notebooklm(synthese, articles_complets)
                    
                    col_nlm1, col_nlm2 = st.columns(2)
                    
                    with col_nlm1:
                        st.download_button(
                            label="üì• T√©l√©charger pour NotebookLM",
                            data=fichier_notebooklm,
                            file_name=f"notebooklm_veille_{datetime.now().strftime('%Y%m%d')}.txt",
                            mime="text/plain",
                            help="Fichier optimis√© pour g√©n√©rer un podcast"
                        )
                    
                    with col_nlm2:
                        st.link_button(
                            label="üîó Ouvrir NotebookLM",
                            url="https://notebooklm.google.com"
                        )
                    
                    sauvegarder_recherche(
                        spec_fr if mode_recherche == "Par sp√©cialit√©" else "Personnalis√©",
                        periode_affichage,
                        type_etude,
                        langue,
                        ids,
                        synthese,
                        mots_cles_originaux
                    )
                    
                    st.success("‚úÖ Sauvegard√© !")
                    
                    st.divider()
                    col1, col2 = st.columns(2)
                    
                    nom = spec_fr if mode_recherche == "Par sp√©cialit√©" else "recherche"
                    
                    with col1:
                        st.download_button(
                            label="üì• TXT",
                            data=synthese,
                            file_name=f"synthese_{nom}.txt",
                            mime="text/plain"
                        )
                    
                    with col2:
                        with st.spinner("üìÑ PDF..."):
                            pdf_bytes = generer_pdf_complet(
                                display_term,
                                periode_affichage,
                                len(ids),
                                ids,
                                synthese,
                                articles_complets
                            )
                        st.download_button(
                            label="üìÑ PDF Complet",
                            data=pdf_bytes,
                            file_name=f"veille_{nom}.pdf",
                            mime="application/pdf"
                        )
                    
                except Exception as e:
                    st.error(f"‚ùå {str(e)}")
        
        except Exception as e:
            st.error(f"‚ùå {str(e)}")

with tab2:
    st.header("üìö Historique")
    
    if not st.session_state.historique:
        st.info("Aucune recherche")
    else:
        for i, rech in enumerate(st.session_state.historique):
            titre = f"üîç {rech['date']} - {rech['specialite']} - {rech['nb_articles']} articles"
            
            with st.expander(titre):
                st.markdown(f"**Sp√©cialit√©:** {rech['specialite']}")
                if rech.get('mots_cles'):
                    st.markdown(f"**Mots-cl√©s:** {rech['mots_cles']}")
                st.markdown(f"**P√©riode:** {rech['periode']}")
                st.markdown(f"**PMIDs:** {', '.join(rech['pmids'])}")
                
                st.divider()
                st.markdown(rech['synthese'])

with tab3:
    st.header("üîó Sources Compl√©mentaires")
    
    st.markdown("### Sources officielles")
    
    for nom, url in SOURCES_SUPPLEMENTAIRES.items():
        st.markdown(f"**{nom}**")
        st.markdown(f"[Acc√©der]({url})")
        st.divider()

st.markdown("---")
st.caption("üíä Veille m√©dicale | PubMed + Gemini 2.5")
```

**Requirements.txt mis √† jour (SANS PyPDF2) :**
```
streamlit
google-generativeai
requests
fpdf
